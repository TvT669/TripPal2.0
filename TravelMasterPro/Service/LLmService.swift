//
//  LLmService.swift
//  TravelMasterPro
//
//  Created by ç ç©†æœ—ç›å°èœœèœ‚ on 2025/8/29.
//

//
//  LLmService.swift
//  TravelMasterPro
//
//  Created by ç ç©†æœ—ç›å°èœœèœ‚ on 2025/8/29.
//

import Foundation

/// é«˜çº§ LLM æœåŠ¡ - æ™ºèƒ½ä½“çš„è¯­è¨€å¤§è„‘
/// æä¾›å®Œæ•´çš„å¤§è¯­è¨€æ¨¡å‹äº¤äº’èƒ½åŠ›ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨ã€æµå¼å“åº”ã€é”™è¯¯é‡è¯•ç­‰


class LLMService {
    var  apiKey: String
    private let baseURL: URL
    private let model: String
    private let urlSession: URLSession
    private let defaultConfig: LLMConfig
    
    // æœåŠ¡ç›‘æ§
    private let serviceMonitor: LLMServiceMonitor
    
    // é‡è¯•é…ç½®
    private let retryConfig: RetryConfig
    
    init() {
        // ä» AIConfig.plist åŠ è½½é…ç½®
        guard let configPath = Bundle.main.path(forResource: "AIConfig", ofType: "plist"),
              let config = NSDictionary(contentsOfFile: configPath) as? [String: Any],
              let apiKey = config["API_KEY"] as? String,
              let baseURLString = config["BASE_URL"] as? String,
              let model = config["MODEL"] as? String else {
            fatalError("AIConfig.plist é…ç½®é”™è¯¯")
        }
        
        self.apiKey = apiKey
        self.baseURL = URL(string: baseURLString)!
        self.model = model
        
        // é»˜è®¤é…ç½®
        self.defaultConfig = LLMConfig(
            maxTokens: config["MAX_TOKENS"] as? Int ?? 4000,
            temperature: config["TEMPERATURE"] as? Double ?? 0.7,
            topP: config["TOP_P"] as? Double ?? 1.0,
            frequencyPenalty: config["FREQUENCY_PENALTY"] as? Double ?? 0.0,
            presencePenalty: config["PRESENCE_PENALTY"] as? Double ?? 0.0
        )
        
        // é‡è¯•é…ç½®
        self.retryConfig = RetryConfig(
            maxRetries: 3,
            baseDelay: 1.0,
            maxDelay: 10.0,
            backoffMultiplier: 2.0
        )
        
        // URL Session é…ç½®
        let sessionConfig = URLSessionConfiguration.default
        sessionConfig.timeoutIntervalForRequest = 120
        sessionConfig.timeoutIntervalForResource = 300
        self.urlSession = URLSession(configuration: sessionConfig)
        
        // æœåŠ¡ç›‘æ§
        self.serviceMonitor = LLMServiceMonitor()
    }
    
    // MARK: - ä¸»è¦æ¥å£æ–¹æ³•
    
    /// æ ‡å‡†å¯¹è¯ - ä¸ä½¿ç”¨å·¥å…·
    func chat(
        messages: [Message],
        systemMessages: [Message]? = nil,
        config: LLMConfig? = nil
    ) async throws -> String {
        let result = try await askTool(
            messages: messages,
            systemMessages: systemMessages,
            tools: nil as [[String: Any]]?, // æ˜ç¡®ç±»å‹
            toolChoice: LLMToolChoice.none, // ä½¿ç”¨å®Œæ•´ç±»å‹å
            config: config
        )
        
        return result.content ?? ""
    }
    
    /// å·¥å…·è°ƒç”¨å¯¹è¯ - æ”¯æŒå·¥å…·ä½¿ç”¨
    func askTool(
        messages: [Message],
        systemMessages: [Message]? = nil,
        tools: [[String: Any]]? = nil,
        toolChoice: LLMToolChoice = .auto, // ä½¿ç”¨é‡å‘½åçš„æšä¸¾
        config: LLMConfig? = nil
    ) async throws -> LLMResponse {
        
        let startTime = Date()
        let requestConfig = config ?? defaultConfig
        
        do {
            let response = try await performRequestWithRetry { [weak self] in
                guard let self = self else { throw LLMError.serviceUnavailable }
                return try await self.performSingleRequest(
                    messages: messages,
                    systemMessages: systemMessages,
                    tools: tools,
                    toolChoice: toolChoice,
                    config: requestConfig
                )
            }
            
            // è®°å½•æˆåŠŸè¯·æ±‚
            let duration = Date().timeIntervalSince(startTime)
            serviceMonitor.recordRequest(
                success: true,
                duration: duration,
                tokenCount: response.usage?.totalTokens ?? 0,
                cost: calculateCost(usage: response.usage)
            )
            
            return response
            
        } catch {
            // è®°å½•å¤±è´¥è¯·æ±‚
            let duration = Date().timeIntervalSince(startTime)
            serviceMonitor.recordRequest(
                success: false,
                duration: duration,
                tokenCount: 0,
                cost: 0.0,
                error: error
            )
            throw error
        }
    }
    
    /// æµå¼å¯¹è¯ - å®æ—¶è¿”å›å“åº”
    func streamChat(
        messages: [Message],
        systemMessages: [Message]? = nil,
        tools: [[String: Any]]? = nil,
        toolChoice: LLMToolChoice = .auto,
        config: LLMConfig? = nil,
        onChunk: @escaping (String) -> Void
    ) async throws -> LLMResponse {
        
        // ç»„åˆæ‰€æœ‰æ¶ˆæ¯
        var allMessages = systemMessages ?? []
        allMessages.append(contentsOf: messages)
        
        let requestConfig = config ?? defaultConfig
        
        // æ„å»ºè¯·æ±‚ä½“
        var requestBody = buildRequestBody(
            messages: allMessages,
            tools: tools,
            toolChoice: toolChoice,
            config: requestConfig
        )
        requestBody["stream"] = true
        
        let url = baseURL.appendingPathComponent("/chat/completions")
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        request.httpBody = try JSONSerialization.data(withJSONObject: requestBody)
        
        let (asyncBytes, response) = try await urlSession.bytes(for: request)
        
        guard let httpResponse = response as? HTTPURLResponse,
              200...299 ~= httpResponse.statusCode else {
            throw LLMError.httpError((response as? HTTPURLResponse)?.statusCode ?? -1)
        }
        
        var fullContent = ""
        var toolCalls: [ToolCall] = []
        var usage: TokenUsage?
        
        for try await line in asyncBytes.lines {
            if line.hasPrefix("data: ") {
                let jsonString = String(line.dropFirst(6))
                
                if jsonString == "[DONE]" { break }
                
                if let data = jsonString.data(using: .utf8),
                   let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
                   let choices = json["choices"] as? [[String: Any]],
                   let firstChoice = choices.first,
                   let delta = firstChoice["delta"] as? [String: Any] {
                    
                    // å¤„ç†å†…å®¹æµ
                    if let content = delta["content"] as? String {
                        fullContent += content
                        onChunk(content)
                    }
                    
                    // å¤„ç†å·¥å…·è°ƒç”¨
                    if let toolCallsData = delta["tool_calls"] as? [[String: Any]] {
                        // å¤„ç†å·¥å…·è°ƒç”¨å¢é‡æ›´æ–°
                        for toolCallData in toolCallsData {
                            // è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦å¤„ç†å¢é‡æ›´æ–°
                            if let id = toolCallData["id"] as? String,
                               let function = toolCallData["function"] as? [String: Any],
                               let name = function["name"] as? String,
                               let arguments = function["arguments"] as? String {
                                // æ„å»ºå®Œæ•´çš„å·¥å…·è°ƒç”¨
                            }
                        }
                    }
                }
                
                // æå–ä½¿ç”¨ç»Ÿè®¡
                if let usageData = (try? JSONSerialization.jsonObject(with: jsonString.data(using: .utf8)!)) as? [String: Any],
                   let usageInfo = usageData["usage"] as? [String: Any] {
                    usage = parseTokenUsage(usageInfo)
                }
            }
        }
        
        return LLMResponse(
            content: fullContent.isEmpty ? nil : fullContent,
            toolCalls: toolCalls.isEmpty ? nil : toolCalls,
            usage: usage
        )
    }

    func completion(messages: [Message]) async throws -> String {
    return try await chat(messages: messages)
}
     func updateApiKey(_ newApiKey: String) {
    self.apiKey = newApiKey
}
     func reconnect() async throws {
        // ç®€å•çš„è¿æ¥æµ‹è¯•
        let testMessage = Message(role: .user, content: "ping")
        _ = try await completion(messages: [testMessage])
    }


    
    // MARK: - æ ¸å¿ƒè¯·æ±‚æ–¹æ³•
    
    private func performSingleRequest(
        messages: [Message],
        systemMessages: [Message]?,
        tools: [[String: Any]]?,
        toolChoice: LLMToolChoice,
        config: LLMConfig
    ) async throws -> LLMResponse {
        
        // ç»„åˆæ‰€æœ‰æ¶ˆæ¯
        var allMessages = systemMessages ?? []
        allMessages.append(contentsOf: messages)
        
        // æ„å»ºè¯·æ±‚ä½“
        let requestBody = buildRequestBody(
            messages: allMessages,
            tools: tools,
            toolChoice: toolChoice,
            config: config
        )
        
        // å‘é€è¯·æ±‚
        let data = try await sendRequest(endpoint: "/chat/completions", body: requestBody)
        
        // è§£æå“åº”
        return try parseResponse(data)
    }
    
    private func buildRequestBody(
        messages: [Message],
        tools: [[String: Any]]?,
        toolChoice: LLMToolChoice,
        config: LLMConfig
    ) -> [String: Any] {
        
        var requestBody: [String: Any] = [
            "model": model,
            "messages": messages.map { formatMessage($0) },
            "max_tokens": config.maxTokens,
            "temperature": config.temperature,
            "top_p": config.topP,
            "frequency_penalty": config.frequencyPenalty,
            "presence_penalty": config.presencePenalty
        ]
        
        if baseURL.absoluteString.contains("deepseek") {
              // DeepSeek å¯èƒ½ä¸æ”¯æŒæŸäº›å‚æ•°
              // åªä¿ç•™åŸºæœ¬å‚æ•°
          } else {
              // OpenAI æ ¼å¼çš„å®Œæ•´å‚æ•°
              requestBody["top_p"] = config.topP
              requestBody["frequency_penalty"] = config.frequencyPenalty
              requestBody["presence_penalty"] = config.presencePenalty
          }
        
        // æ·»åŠ å·¥å…·é…ç½®
        if let tools = tools {
            requestBody["tools"] = tools
            
            switch toolChoice {
            case .auto:
                requestBody["tool_choice"] = "auto"
            case .required:
                requestBody["tool_choice"] = "required"
            case .none:
                requestBody["tool_choice"] = "none"
            case .specific(let toolName):
                requestBody["tool_choice"] = [
                    "type": "function",
                    "function": ["name": toolName]
                ]
            }
        }
        
        return requestBody
    }
    
    private func parseResponse(_ data: Data) throws -> LLMResponse {
        guard let responseDict = try JSONSerialization.jsonObject(with: data) as? [String: Any] else {
            throw LLMError.invalidResponse
        }
        
        // æ£€æŸ¥é”™è¯¯
        if let error = responseDict["error"] as? [String: Any],
           let message = error["message"] as? String {
            throw LLMError.apiError(message)
        }
        
        guard let choices = responseDict["choices"] as? [[String: Any]],
              let firstChoice = choices.first,
              let message = firstChoice["message"] as? [String: Any] else {
            throw LLMError.invalidResponse
        }
        
        // æå–å†…å®¹å’Œå·¥å…·è°ƒç”¨
        let content = message["content"] as? String
        let toolCallsData = message["tool_calls"] as? [[String: Any]]
        
        // è§£æå·¥å…·è°ƒç”¨
        var toolCalls: [ToolCall]?
        if let toolCallsData = toolCallsData, !toolCallsData.isEmpty {
            toolCalls = try toolCallsData.map { callData in
                guard let id = callData["id"] as? String,
                      let function = callData["function"] as? [String: Any],
                      let name = function["name"] as? String,
                      let argumentsString = function["arguments"] as? String else {
                    throw LLMError.invalidToolCall
                }
                
                let arguments: [String: String]
                if let argsData = argumentsString.data(using: .utf8),
                   let parsedArgs = try? JSONSerialization.jsonObject(with: argsData) as? [String: Any] {
                       arguments = parsedArgs.compactMapValues { value in
                                       if let stringValue = value as? String {
                                           return stringValue
                                       } else if let numberValue = value as? NSNumber {
                                           return numberValue.stringValue
                                       } else {
                                           return String(describing: value)
                                       }
                                   }
                } else {
                    arguments = [:]
                }
                
                return ToolCall(
                    id: id,
                    function: ToolCall.ToolFunction(
                        name: name,
                        arguments: arguments
                    )
                )
            }
        }
        
        // è§£æä½¿ç”¨ç»Ÿè®¡
        let usage = responseDict["usage"] as? [String: Any]
        let tokenUsage = usage != nil ? parseTokenUsage(usage!) : nil
        
        return LLMResponse(
            content: content,
            toolCalls: toolCalls,
            usage: tokenUsage
        )
    }
    
    private func parseTokenUsage(_ usageData: [String: Any]) -> TokenUsage {
        return TokenUsage(
            promptTokens: usageData["prompt_tokens"] as? Int ?? 0,
            completionTokens: usageData["completion_tokens"] as? Int ?? 0,
            totalTokens: usageData["total_tokens"] as? Int ?? 0
        )
    }
    
    // MARK: - é‡è¯•æœºåˆ¶
    
    private func performRequestWithRetry<T>(
        _ operation: @escaping () async throws -> T
    ) async throws -> T {
        
        var lastError: Error?
        
        for attempt in 0...retryConfig.maxRetries {
            do {
                return try await operation()
            } catch {
                lastError = error
                
                // æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
                if !shouldRetry(error: error, attempt: attempt) {
                    throw error
                }
                
                // è®¡ç®—å»¶è¿Ÿæ—¶é—´
                let delay = calculateRetryDelay(attempt: attempt)
                try await Task.sleep(nanoseconds: UInt64(delay * 1_000_000_000))
            }
        }
        
        throw lastError ?? LLMError.maxRetriesExceeded
    }
    
    private func shouldRetry(error: Error, attempt: Int) -> Bool {
        guard attempt < retryConfig.maxRetries else { return false }
        
        // æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦é‡è¯•
        if let llmError = error as? LLMError {
            switch llmError {
            case .networkError, .timeout, .serviceUnavailable:
                return true
            case .httpError(let code):
                return code >= 500 || code == 429 // æœåŠ¡å™¨é”™è¯¯æˆ–é™æµ
            default:
                return false
            }
        }
        
        if let urlError = error as? URLError {
            switch urlError.code {
            case .timedOut, .networkConnectionLost, .notConnectedToInternet:
                return true
            default:
                return false
            }
        }
        
        return false
    }
    
    private func calculateRetryDelay(attempt: Int) -> Double {
        let exponentialDelay = retryConfig.baseDelay * pow(retryConfig.backoffMultiplier, Double(attempt))
        let jitter = Double.random(in: 0.8...1.2) // æ·»åŠ æŠ–åŠ¨
        return min(exponentialDelay * jitter, retryConfig.maxDelay)
    }
    
    // MARK: - è¾…åŠ©æ–¹æ³•
    
    private func sendRequest(endpoint: String, body: [String: Any]) async throws -> Data {
        let url = baseURL.appendingPathComponent(endpoint)
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        
        let requestData = try JSONSerialization.data(withJSONObject: body)
        request.httpBody = requestData
        
        // âœ… æ·»åŠ è°ƒè¯•æ—¥å¿—
        print("ğŸ” è¯·æ±‚ URL: \(url)")
        print("ğŸ” è¯·æ±‚å¤´: \(request.allHTTPHeaderFields ?? [:])")
        print("ğŸ” è¯·æ±‚ä½“: \(String(data: requestData, encoding: .utf8) ?? "æ— æ³•è§£æ")")
        
        do {
            let (data, response) = try await urlSession.data(for: request)
            
            guard let httpResponse = response as? HTTPURLResponse else {
                throw LLMError.invalidResponse
            }
            
            // âœ… æ·»åŠ å“åº”æ—¥å¿—
            print("ğŸ” å“åº”çŠ¶æ€ç : \(httpResponse.statusCode)")
            print("ğŸ” å“åº”å¤´: \(httpResponse.allHeaderFields)")
            
            guard 200...299 ~= httpResponse.statusCode else {
                let errorMessage = String(data: data, encoding: .utf8) ?? "Unknown error"
                print("âŒ é”™è¯¯å“åº”ä½“: \(errorMessage)")
                throw LLMError.httpError(httpResponse.statusCode)
            }
            
            print("âœ… æˆåŠŸå“åº”: \(String(data: data, encoding: .utf8)?.prefix(200) ?? "æ— æ³•è§£æ")")
            return data
            
        } catch {
            print("âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: \(error)")
            if error is URLError {
                throw LLMError.networkError
            }
            throw error
        }
    }
    
    private func formatMessage(_ message: Message) -> [String: Any] {
        var formattedMessage: [String: Any] = [
            "role": message.role.rawValue,
            "content": message.content
        ]
        
        // âœ… å¤„ç† assistant æ¶ˆæ¯ä¸­çš„å·¥å…·è°ƒç”¨
        if message.role == .assistant,
           let metadata = message.metadata,
           let toolCalls = metadata.toolCalls {
            
            let formattedToolCalls = toolCalls.map { toolCall in
                return [
                    "id": toolCall.id,
                    "type": "function",
                    "function": [
                        "name": toolCall.function.name,
                        "arguments": convertArgumentsToJSON(toolCall.function.arguments)
                    ]
                ]
            }
            formattedMessage["tool_calls"] = formattedToolCalls
        }
        
        // âœ… å¤„ç† tool æ¶ˆæ¯
        if message.role == .tool {
            if let toolCallId = message.metadata?.toolCallId {
                formattedMessage["tool_call_id"] = toolCallId
            }
            if let toolName = message.metadata?.toolName {
                formattedMessage["name"] = toolName
            }
        }
        
        // å¤„ç†å›¾ç‰‡é™„ä»¶
        if let attachments = message.metadata?.attachments,
           !attachments.isEmpty {
            let imageAttachments = attachments.filter { $0.type == .image }
            if !imageAttachments.isEmpty {
                var contentArray: [[String: Any]] = [
                    ["type": "text", "text": message.content]
                ]
                
                for attachment in imageAttachments {
                    contentArray.append([
                        "type": "image_url",
                        "image_url": [
                            "url": "data:\(attachment.mimeType ?? "image/jpeg");base64,\(attachment.data)"
                        ]
                    ])
                }
                formattedMessage["content"] = contentArray
            }
        }
        
        return formattedMessage
    }

    // âœ… æ·»åŠ å‚æ•°è½¬æ¢æ–¹æ³•
    private func convertArgumentsToJSON(_ arguments: [String: String]) -> String {
        do {
            let data = try JSONSerialization.data(withJSONObject: arguments)
            return String(data: data, encoding: .utf8) ?? "{}"
        } catch {
            return "{}"
        }
    }
    
    private func calculateCost(usage: TokenUsage?) -> Double {
        guard let usage = usage else { return 0.0 }
        
        // è¿™é‡Œæ ¹æ®å®é™…çš„æ¨¡å‹å®šä»·è®¡ç®—æˆæœ¬
        // ä»¥ GPT-4 ä¸ºä¾‹çš„ç®€åŒ–è®¡ç®—
        let inputCostPer1K = 0.03  // $0.03 per 1K tokens
        let outputCostPer1K = 0.06 // $0.06 per 1K tokens
        
        let inputCost = Double(usage.promptTokens) / 1000.0 * inputCostPer1K
        let outputCost = Double(usage.completionTokens) / 1000.0 * outputCostPer1K
        
        return inputCost + outputCost
    }
    
    // MARK: - æœåŠ¡çŠ¶æ€
    
    func getServiceStatus() -> LLMServiceStatus {
        return serviceMonitor.getStatus()
    }
    
    func resetStatistics() {
        serviceMonitor.reset()
    }
}

// MARK: - é…ç½®å’Œæ•°æ®æ¨¡å‹

/// LLM é…ç½®
struct LLMConfig {
    let maxTokens: Int
    let temperature: Double
    let topP: Double
    let frequencyPenalty: Double
    let presencePenalty: Double
    
    static let `default` = LLMConfig(
        maxTokens: 4000,
        temperature: 0.7,
        topP: 1.0,
        frequencyPenalty: 0.0,
        presencePenalty: 0.0
    )
    
    static let creative = LLMConfig(
        maxTokens: 4000,
        temperature: 1.0,
        topP: 0.9,
        frequencyPenalty: 0.1,
        presencePenalty: 0.1
    )
    
    static let precise = LLMConfig(
        maxTokens: 4000,
        temperature: 0.1,
        topP: 0.95,
        frequencyPenalty: 0.0,
        presencePenalty: 0.0
    )
}

/// é‡è¯•é…ç½®
struct RetryConfig {
    let maxRetries: Int
    let baseDelay: Double
    let maxDelay: Double
    let backoffMultiplier: Double
}

/// LLM å“åº”
struct LLMResponse {
    let content: String?
    let toolCalls: [ToolCall]?
    let usage: TokenUsage?
}

/// Token ä½¿ç”¨ç»Ÿè®¡
struct TokenUsage {
    let promptTokens: Int
    let completionTokens: Int
    let totalTokens: Int
}

/// LLM ä¸“ç”¨çš„å·¥å…·é€‰æ‹©æšä¸¾ (é‡å‘½åé¿å…å†²çª)
enum LLMToolChoice {
    case auto
    case required
    case none
    case specific(String)
}

/// LLM é”™è¯¯ç±»å‹
enum LLMError: Error, LocalizedError {
    case networkError
    case timeout
    case serviceUnavailable
    case httpError(Int)
    case apiError(String)
    case invalidResponse
    case invalidToolCall
    case maxRetriesExceeded
    
    var errorDescription: String? {
        switch self {
        case .networkError:
            return "ç½‘ç»œè¿æ¥é”™è¯¯"
        case .timeout:
            return "è¯·æ±‚è¶…æ—¶"
        case .serviceUnavailable:
            return "æœåŠ¡ä¸å¯ç”¨"
        case .httpError(let code):
            return "HTTPé”™è¯¯: \(code)"
        case .apiError(let message):
            return "APIé”™è¯¯: \(message)"
        case .invalidResponse:
            return "æ— æ•ˆå“åº”"
        case .invalidToolCall:
            return "æ— æ•ˆå·¥å…·è°ƒç”¨"
        case .maxRetriesExceeded:
            return "è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°"
        }
    }
}

// MARK: - æœåŠ¡ç›‘æ§

/// LLM æœåŠ¡ç›‘æ§å™¨
class LLMServiceMonitor {
    private var requestCount: Int = 0
    private var successCount: Int = 0
    private var totalDuration: TimeInterval = 0
    private var totalTokens: Int = 0
    private var totalCost: Double = 0
    private var lastErrors: [Error] = []
    private let maxErrorHistory = 10
    
    private let queue = DispatchQueue(label: "llm.monitor", attributes: .concurrent)
    
    func recordRequest(
        success: Bool,
        duration: TimeInterval,
        tokenCount: Int,
        cost: Double,
        error: Error? = nil
    ) {
        queue.async(flags: .barrier) {
            self.requestCount += 1
            self.totalDuration += duration
            self.totalTokens += tokenCount
            self.totalCost += cost
            
            if success {
                self.successCount += 1
            } else if let error = error {
                self.lastErrors.append(error)
                if self.lastErrors.count > self.maxErrorHistory {
                    self.lastErrors.removeFirst()
                }
            }
        }
    }
    
    func getStatus() -> LLMServiceStatus {
        return queue.sync {
            return LLMServiceStatus(
                requestCount: requestCount,
                successRate: requestCount > 0 ? Double(successCount) / Double(requestCount) : 0,
                averageResponseTime: requestCount > 0 ? totalDuration / Double(requestCount) : 0,
                totalTokens: totalTokens,
                totalCost: totalCost,
                recentErrors: Array(lastErrors.suffix(5))
            )
        }
    }
    
    func reset() {
        queue.async(flags: .barrier) {
            self.requestCount = 0
            self.successCount = 0
            self.totalDuration = 0
            self.totalTokens = 0
            self.totalCost = 0
            self.lastErrors.removeAll()
        }
    }
}

/// LLM æœåŠ¡çŠ¶æ€
struct LLMServiceStatus {
    let requestCount: Int
    let successRate: Double
    let averageResponseTime: TimeInterval
    let totalTokens: Int
    let totalCost: Double
    let recentErrors: [Error]
    
    var formattedStatus: String {
        return """
        ğŸ“Š LLMæœåŠ¡çŠ¶æ€ï¼š
        â€¢ è¯·æ±‚æ¬¡æ•°ï¼š\(requestCount)
        â€¢ æˆåŠŸç‡ï¼š\(String(format: "%.1f%%", successRate * 100))
        â€¢ å¹³å‡å“åº”æ—¶é—´ï¼š\(String(format: "%.2fs", averageResponseTime))
        â€¢ æ€»Tokenæ•°ï¼š\(totalTokens)
        â€¢ æ€»æˆæœ¬ï¼š$\(String(format: "%.4f", totalCost))
        â€¢ æœ€è¿‘é”™è¯¯ï¼š\(recentErrors.count)ä¸ª
        """
    }
}


extension LLMService {
    
    /// æ™ºèƒ½å·¥å…·è°ƒç”¨ - å¤„ç†å®Œæ•´çš„å·¥å…·è°ƒç”¨æµç¨‹
    func thinkAndAct(
        messages: [Message],
        availableTools: [Tool],
        config: LLMConfig? = nil
    ) async throws -> LLMResponse {
        
        var conversationMessages = messages
        var iterationCount = 0
        let maxIterations = 5
        let requestConfig = config ?? defaultConfig
        
        while iterationCount < maxIterations {
            iterationCount += 1
            print("ğŸ”„ å·¥å…·è°ƒç”¨è¿­ä»£ \(iterationCount)")
            
            // å‡†å¤‡å·¥å…·å‚æ•°
            let toolParameters = availableTools.map { $0.toParameters() }
            
            // éªŒè¯æ¶ˆæ¯åºåˆ—
            try validateMessageSequence(conversationMessages)
            
            // å‘é€è¯·æ±‚ï¼ˆåŒ…å«å·¥å…·ï¼‰
            let response = try await askTool(
                messages: conversationMessages,
                tools: toolParameters.isEmpty ? nil : toolParameters,
                toolChoice: toolParameters.isEmpty ? .none : .auto,
                config: requestConfig
            )
            
            // æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            guard let toolCalls = response.toolCalls, !toolCalls.isEmpty else {
                // æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å“åº”
                return response
            }
            
            // âœ… å…³é”®ä¿®å¤ï¼šæ·»åŠ  assistant æ¶ˆæ¯ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ï¼‰
            // æ³¨æ„ï¼šå½“æœ‰å·¥å…·è°ƒç”¨æ—¶ï¼Œä¸ä¿å­˜æ–‡æœ¬å†…å®¹ï¼ˆé¿å…æ˜¾ç¤º DSML æ ‡è®°ï¼‰
            let assistantMessage = Message(
                id: UUID().uuidString,
                role: .assistant,
                content: "", // å·¥å…·è°ƒç”¨æ—¶ä½¿ç”¨ç©ºå­—ç¬¦ä¸²,é¿å…æ˜¾ç¤º DSML
                metadata: MessageMetadata().with(toolCalls: toolCalls)
            )
            conversationMessages.append(assistantMessage)
            
            // æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶æ·»åŠ  tool æ¶ˆæ¯
            var allToolResults: [String] = []
            
            for toolCall in toolCalls {
                do {
                    print("ğŸ”§ æ‰§è¡Œå·¥å…·: \(toolCall.function.name)")
                    let toolResult = try await executeToolCall(toolCall, tools: availableTools)
                    
                    // âœ… ä¸ºæ¯ä¸ªå·¥å…·è°ƒç”¨æ·»åŠ å•ç‹¬çš„ tool æ¶ˆæ¯
                    let toolMessage = Message(
                        id: UUID().uuidString,
                        role: .tool,
                        content: toolResult.output ?? toolResult.error ?? "å·¥å…·æ‰§è¡Œå®Œæˆ",
                        metadata: MessageMetadata().with(
                            toolCallId: toolCall.id,
                            toolName: toolCall.function.name
                        )
                    )
                    conversationMessages.append(toolMessage)
                    
                    // æ”¶é›†ç»“æœ
                    if let output = toolResult.output {
                        allToolResults.append("ã€\(toolCall.function.name)ã€‘\n\(output)")
                    } else if let error = toolResult.error {
                        allToolResults.append("ã€\(toolCall.function.name) é”™è¯¯ã€‘\n\(error)")
                    }
                    
                } catch {
                    print("âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: \(error)")
                    
                    // å³ä½¿å·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œä¹Ÿè¦æ·»åŠ  tool æ¶ˆæ¯
                    let errorMessage = Message(
                        id: UUID().uuidString,
                        role: .tool,
                        content: "å·¥å…·æ‰§è¡Œå¤±è´¥: \(error.localizedDescription)",
                        metadata: MessageMetadata().with(
                            toolCallId: toolCall.id,
                            toolName: toolCall.function.name
                        )
                    )
                    conversationMessages.append(errorMessage)
                    
                    allToolResults.append("ã€\(toolCall.function.name) é”™è¯¯ã€‘\nå·¥å…·æ‰§è¡Œå¤±è´¥: \(error.localizedDescription)")
                }
            }
            
            // å‘é€æœ€ç»ˆè¯·æ±‚è·å–æ€»ç»“å“åº”ï¼ˆä¸åŒ…å«å·¥å…·ï¼‰
            let finalResponse = try await askTool(
                messages: conversationMessages,
                tools: nil,
                toolChoice: .none,
                config: requestConfig
            )
            
            // è¿”å›åŒ…å«å·¥å…·ç»“æœçš„æœ€ç»ˆå“åº”
            return LLMResponse(
                content: finalResponse.content ?? allToolResults.joined(separator: "\n\n"),
                toolCalls: nil,
                usage: finalResponse.usage
            )
        }
        
        throw LLMError.maxRetriesExceeded
    }
    
    /// æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨
    private func executeToolCall(_ toolCall: ToolCall, tools: [Tool]) async throws -> ToolResult {
        // æ‰¾åˆ°å¯¹åº”çš„å·¥å…·
        guard let tool = tools.first(where: { $0.name == toolCall.function.name }) else {
            throw LLMError.apiError("æœªæ‰¾åˆ°å·¥å…·: \(toolCall.function.name)")
        }
        
        // è½¬æ¢å‚æ•°æ ¼å¼
        let arguments = toolCall.function.arguments.reduce(into: [String: Any]()) { result, pair in
            result[pair.key] = pair.value
        }
        
        // æ‰§è¡Œå·¥å…·
        return try await tool.execute(arguments: arguments)
    }
    
    /// éªŒè¯æ¶ˆæ¯åºåˆ—
    private func validateMessageSequence(_ messages: [Message]) throws {
        for i in 0..<messages.count {
            let message = messages[i]
            
            if message.role == .tool {
                // tool æ¶ˆæ¯å¿…é¡»è·Ÿåœ¨åŒ…å« tool_calls çš„ assistant æ¶ˆæ¯åé¢
                guard i > 0 else {
                    throw LLMError.apiError("tool æ¶ˆæ¯ä¸èƒ½æ˜¯ç¬¬ä¸€æ¡æ¶ˆæ¯")
                }
                
                let previousMessage = messages[i - 1]
                guard previousMessage.role == .assistant else {
                    throw LLMError.apiError("tool æ¶ˆæ¯å¿…é¡»è·Ÿåœ¨ assistant æ¶ˆæ¯åé¢")
                }
                
                // éªŒè¯ tool_call_id å­˜åœ¨
                guard message.metadata?.toolCallId != nil else {
                    throw LLMError.apiError("tool æ¶ˆæ¯å¿…é¡»åŒ…å« tool_call_id")
                }
            }
        }
    }
}


